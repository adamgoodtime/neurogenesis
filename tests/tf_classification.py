# # -*- coding: utf-8 -*-
# """Classification with Tensorflow.ipynb
#
# Automatically generated by Colaboratory.
#
# Original file is located at
#     https://colab.research.google.com/drive/1vs2Zhm0WFjTDdvrgbHNaK6QCi6Sq2f9Q
# """
#
# import numpy as np
# import pandas as pd
# import plotly.express as px
# import plotly.graph_objects as go
#
# test = 'wine'
# if test == 'wine':
#     from wine_data import *
#     num_outputs = 3
#     train_labels = training_set_labels
#     train_feat = training_set_wines
#     test_labels = test_set_labels
#     test_feat = test_set_wines
#     X = train_feat + test_feat  #pd.DataFrame(train_feat + test_feat)
#     y = train_labels + test_labels  #pd.DataFrame(train_labels + test_labels)
# else:
#     df = pd.read_csv('../datasets/winequalityN.csv')
#     df.sample(5)
#     df.isna().sum()
#     df = df.dropna()
#     df.isna().sum()
#     df['is_white_wine'] = [
#         1 if typ == 'white' else 0 for typ in df['type']
#     ]
#     df.head()
#     white = df[df['type']=='white']
#     red = df[df['type'] == 'red']
#     from plotly.subplots import make_subplots
#     fig = make_subplots(rows=1, cols=2,
#                         column_widths=[0.45, 0.45],
#                         subplot_titles=['White Wine Quality', 'Red Wine Quality'])
#     fig.append_trace(go.Bar(x=white['quality'].value_counts().index,
#                             y=white['quality'].value_counts(),
#                             text = white['quality'].value_counts(),
#                             marker=dict(
#                             color='snow',
#                             line=dict(color='white', width=1)
#                     ),
#                             name=''
#                    ), 1,1
#                  )
#     fig.append_trace(go.Bar(x=red['quality'].value_counts().index,
#                             y=red['quality'].value_counts(),
#                             text=red['quality'].value_counts(),
#                             marker=dict(
#                             color='coral',
#                             line=dict(color='red', width=1)
#                     ),
#                             name=''
#                    ), 1,2
#                  )
#     fig.update_traces(textposition='outside')
#     fig.update_layout(margin={'b':0,'l':0,'r':0,'t':100},
#         paper_bgcolor='rgb(248, 248, 255)',
#         plot_bgcolor='rgb(248, 248, 255)',
#         showlegend=False,
#         title = {'font': {
#                             'family':'monospace',
#                             'size': 22,
#                             'color':'grey'},
#                 'text':'Quality Distribution In Red & White Wine',
#                 'x':0.50,'y':1})
#     fig.show()
#     df['is_good_wine'] = [
#         1 if quality >= 6 else 0 for quality in df['quality']
#     ]
#     df.drop('quality', axis=1, inplace=True)
#     df.drop('type', axis=1, inplace=True)
#     df.head()
#     X = df.drop('is_good_wine', axis=1)
#     y = df['is_good_wine']
#
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y,
#     test_size=0.2, random_state=42
# )
# # X_train,y_train
#
# from sklearn.preprocessing import StandardScaler
#
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)
# # X_train
#
# import tensorflow as tf
# tf.random.set_seed(42)
#
#
# model = tf.keras.Sequential([
#     tf.keras.layers.Dense(64, activation='relu'),
#     tf.keras.layers.Dense(64, activation='relu'),
#     tf.keras.layers.Dense(64, activation='relu'),
#     tf.keras.layers.Dense(1, activation='sigmoid')
# ])
#
# model.compile(
#     loss=tf.keras.losses.categorical_crossentropy,
#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.03),
#     metrics=[
#         tf.keras.metrics.Accuracy(name='accuracy'),
#         tf.keras.metrics.Precision(name='precision'),
#         tf.keras.metrics.Recall(name='recall')
#     ]
# )
#
# history = model.fit(X_train_scaled, np.array(y_train), epochs=100)
#
# import matplotlib.pyplot as plt
# from matplotlib import rcParams
#
# rcParams['figure.figsize'] = (18, 8)
# rcParams['axes.spines.top'] = False
# rcParams['axes.spines.right'] = False
#
# plt.plot(
#     np.arange(1, 101),
#     history.history['loss'], label='Loss'
# )
# plt.plot(
#     np.arange(1, 101),
#     history.history['accuracy'], label='Accuracy'
# )
# plt.plot(
#     np.arange(1, 101),
#     history.history['precision'], label='Precision'
# )
# plt.plot(
#     np.arange(1, 101),
#     history.history['recall'], label='Recall'
# )
# plt.title('Evaluation metrics', size=20)
# plt.xlabel('Epoch', size=14)
# plt.legend()
#
# predictions = model.predict(X_test_scaled)
# print(predictions)
#
# from sklearn.metrics import roc_curve
# from sklearn.metrics import auc
# from sklearn.metrics import roc_auc_score
#
# def plot_roc_curve(fpr, tpr):
#     plt.plot(fpr, tpr, color='orange', label='ROC')
#     plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
#     plt.xlabel('False Positive Rate')
#     plt.ylabel('True Positive Rate')
#     plt.title('Receiver Operating Characteristic (ROC) Curve')
#     plt.legend()
#     plt.show()
#
# # Computing manually fpr, tpr, thresholds and roc auc
# # fpr, tpr, thresholds = roc_curve(np.reshape(np.array(y_test), [36,1]), predictions)
# # y_test = y_test.squeeze(axis=1)
# # fpr, tpr, thresholds = roc_curve(y_test, predictions)
# # roc_auc = auc(fpr, tpr)
# #
# # print("ROC_AUC Score : ",roc_auc)
# # print("Function for ROC_AUC Score : ",roc_auc_score(y_test, predictions))
# #
# # optimal_idx = np.argmax(tpr - fpr)
# # optimal_threshold = thresholds[optimal_idx]
# # print("Threshold value is:", optimal_threshold)
# # plot_roc_curve(fpr, tpr)
# #
# prediction_classes = np.reshape(predictions, [1, len(predictions)]).tolist()[0]#[
#     # 1 if prob > optimal_threshold else 0 for prob in np.ravel(predictions)
# # ]
# print(prediction_classes[:20])
#
# from sklearn.metrics import confusion_matrix
#
# print(confusion_matrix(y_test, prediction_classes))
#
# from sklearn.metrics import accuracy_score, precision_score, recall_score
# average = "macro"
# print(f'Accuracy: {accuracy_score(y_test, prediction_classes):.2f}')
# print(f'Precision: {precision_score(y_test, prediction_classes, average=average):.2f}')
# print(f'Recall: {recall_score(y_test, prediction_classes, average=average):.2f}')
#
# print('done')

# multi-class classification with Keras
import pandas
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras import utils as np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedShuffleSplit, LeaveOneOut, StratifiedKFold
from tensorflow.python.keras.callbacks import LambdaCallback
import numpy as np

# load dataset
test = 'breast'
if test == 'breast':
    from breast_data import *
    num_outputs = 2
    train_labels = training_set_labels
    train_feat = training_set_breasts
    num_inputs = len(train_feat[0])
    test_labels = test_set_labels
    test_feat = test_set_breasts
    retest_rate = 1
    retest_size = len(test_set_labels)
    X = np.array(train_feat + test_feat)
    Y = np.array(train_labels + test_labels)
    # X = pandas.DataFrame(train_feat + test_feat)
    # Y = pandas.DataFrame(train_labels + test_labels)
elif test == 'wine':
    from wine_data import *
    num_inputs = 13
    num_outputs = 3
    train_labels = training_set_labels
    train_feat = training_set_wines
    test_labels = test_set_labels
    test_feat = test_set_wines
    X = np.array(train_feat + test_feat)
    Y = np.array(train_labels + test_labels)
    # X = pandas.DataFrame(train_feat + test_feat)
    # Y = pandas.DataFrame(train_labels + test_labels)
else:
    dataframe = pandas.read_csv("../datasets/iris.data", header=None)
    dataset = dataframe.values
    X = dataset[:,0:4].astype(float)
    Y = dataset[:,4]
    num_inputs = 4
    num_outputs = 3
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

# define baseline model
def baseline_model(n_neurons, lr):
    # create model
    model = Sequential()
    model.add(Dense(n_neurons, input_dim=num_inputs, activation='relu'))
    model.add(Dense(num_outputs, activation='softmax'))
    # Compile model
    optimizer = tf.keras.optimizers.Adam(lr=lr)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=1, verbose=1)
# kfold = KFold(n_splits=10, shuffle=True)
#
# results = baseline_model.evaluate(x_test, y_test, batch_size=128)
# print("test loss, test acc:", results)
#
# predictions = baseline_model.predict(x_test[:3])
# print("predictions shape:", predictions.shape)
#
# batch_print_callback = LambdaCallback(
#     on_batch_begin=cross_val_score(estimator, X[:20], dummy_y))
# results = cross_val_score(estimator, X, dummy_y, cv=kfold, )
# print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
#
# cleanup_callback = LambdaCallback(on_batch_end=lambda logs: p.terminate())
#
# cbks = [cleanup_callback]



splits = 10
testing_data = [[] for i in range(splits)]

sss = StratifiedKFold(n_splits=splits, random_state=2727, shuffle=True)
for repeat, (train_index, test_index) in enumerate(sss.split(X, Y)):
    net = baseline_model()
    retest_callback = LambdaCallback(
        on_batch_end=lambda batch, logs:
        # print('\n', net.predict(X[test_index]), 'arged --Â¬ \n',
        #       [np.argmax(a) for a in net.predict(X[test_index])], 'then \n', Y[test_index]))
        testing_data[repeat].append(
            np.average([np.argmax(a) == b for a, b in zip(net.predict(X[test_index]), Y[test_index])])))
        # print('\ntest accuracy:',
        #       np.average([np.argmax(a) == b for a, b in zip(net.predict(X[test_index]), Y[test_index])])))
    print("training model for repeat", repeat)
    # scce = tf.keras.losses.sparse_categorical_crossentropy(dummy_y[test_index], net.predict(X))
    history = net.fit(X[train_index], dummy_y[train_index],
                      batch_size=1, validation_data=(X[test_index], dummy_y[test_index]),
                      callbacks=retest_callback,
                      epochs=4, verbose=True)
    # scce2 = tf.keras.losses.sparse_categorical_crossentropy(dummy_y[test_index], net.predict(X))

ave_test = []
for j in range(len(testing_data[0])):
    total = 0.
    for i in range(len(testing_data)):
        total += testing_data[i][j]
    ave_test.append(total / len(testing_data))
print('Done')